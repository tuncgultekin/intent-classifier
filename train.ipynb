{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UltimateAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfuM_mFAED7N",
        "colab_type": "text"
      },
      "source": [
        "# Install Required Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YP5MbJIEG74",
        "colab_type": "code",
        "outputId": "47121d88-3891-4e64-b83e-e1bd6aa0843d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install numpy==1.17.3\n",
        "!pip install pandas==0.25.3\n",
        "!pip install nltk==3.2.5\n",
        "!pip install keras==2.2.5\n",
        "!pip install tensorflow==1.15.0\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.17.3 in /usr/local/lib/python3.6/dist-packages (1.17.3)\n",
            "Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas==0.25.3) (1.12.0)\n",
            "Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.2.5) (1.12.0)\n",
            "Requirement already satisfied: keras==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.1.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq7kMRbSCCzD",
        "colab_type": "text"
      },
      "source": [
        "# Load Train & Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ITtI9fzB8wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv(\"train.tsv\", sep=\"\\t\" ,names=[\"text\",\"intent\"])\n",
        "test = pd.read_csv(\"test.tsv\", sep=\"\\t\",names=[\"text\",\"intent\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3aMgePoCsaE",
        "colab_type": "code",
        "outputId": "09cd0ef9-3fe9-4d7d-8cf4-d1571a6938a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4634</td>\n",
              "      <td>4634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>4634</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>all flights from boston to washington dc on th...</td>\n",
              "      <td>flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>3426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  intent\n",
              "count                                                4634    4634\n",
              "unique                                               4634      22\n",
              "top     all flights from boston to washington dc on th...  flight\n",
              "freq                                                    1    3426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq7RuPkJDO2T",
        "colab_type": "code",
        "outputId": "356a9a75-ed32-4cd7-83be-4ddb795e171d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>850</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>show me ground transportation in phoenix</td>\n",
              "      <td>flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>613</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text  intent\n",
              "count                                        850     850\n",
              "unique                                       850      20\n",
              "top     show me ground transportation in phoenix  flight\n",
              "freq                                           1     613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll-DipAHDV_T",
        "colab_type": "code",
        "outputId": "f1f2349d-f6fc-40c4-cba2-db9d8af77c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "training_classes = np.unique(train['intent'])\n",
        "test_classes = np.unique(test['intent'])\n",
        "\n",
        "print(\"Unique intent classes in training set: {0}\".format(training_classes))\n",
        "print(\"Unique intent classes in test set: {0}\".format(test_classes))\n",
        "\n",
        "in_test_but_notin_train = [(t if t not in training_classes else '') for t in test_classes]\n",
        "in_train_but_notin_test = [(t if t not in test_classes else '') for t in training_classes]\n",
        "\n",
        "print(in_test_but_notin_train)\n",
        "print(in_train_but_notin_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique intent classes in training set: ['abbreviation' 'aircraft' 'aircraft+flight+flight_no' 'airfare'\n",
            " 'airfare+flight_time' 'airline' 'airline+flight_no' 'airport' 'capacity'\n",
            " 'cheapest' 'city' 'distance' 'flight' 'flight+airfare' 'flight_no'\n",
            " 'flight_time' 'ground_fare' 'ground_service' 'ground_service+ground_fare'\n",
            " 'meal' 'quantity' 'restriction']\n",
            "Unique intent classes in test set: ['abbreviation' 'aircraft' 'airfare' 'airfare+flight' 'airline' 'airport'\n",
            " 'capacity' 'city' 'day_name' 'distance' 'flight' 'flight+airfare'\n",
            " 'flight+airline' 'flight_no' 'flight_no+airline' 'flight_time'\n",
            " 'ground_fare' 'ground_service' 'meal' 'quantity']\n",
            "['', '', '', 'airfare+flight', '', '', '', '', 'day_name', '', '', '', 'flight+airline', '', 'flight_no+airline', '', '', '', '', '']\n",
            "['', '', 'aircraft+flight+flight_no', '', 'airfare+flight_time', '', 'airline+flight_no', '', '', 'cheapest', '', '', '', '', '', '', '', '', 'ground_service+ground_fare', '', '', 'restriction']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9CDvT7uZgCz",
        "colab_type": "code",
        "outputId": "56cd9df2-dad8-462f-8828-3dc6430a2c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Extract and Merge all class labels\n",
        "# Test set contains an additional class which is 'day_name' \n",
        "def extract_classes(arr, all_classes=[]):\n",
        "  for t in arr:\n",
        "    if ('+' in t):\n",
        "      classes = t.split('+')\n",
        "      for c in classes:\n",
        "        if (c not in all_classes):\n",
        "          all_classes.append(c)\n",
        "    elif (t not in all_classes):\n",
        "      all_classes.append(t)\n",
        "  \n",
        "  return all_classes\n",
        "\n",
        "all_classes = extract_classes(training_classes)\n",
        "all_classes = np.array(extract_classes(test_classes, all_classes = all_classes))\n",
        "all_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['abbreviation', 'aircraft', 'flight', 'flight_no', 'airfare',\n",
              "       'flight_time', 'airline', 'airport', 'capacity', 'cheapest',\n",
              "       'city', 'distance', 'ground_fare', 'ground_service', 'meal',\n",
              "       'quantity', 'restriction', 'day_name'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "allfEAkYD5gR",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvGP8wZgFUoH",
        "colab_type": "code",
        "outputId": "448e709b-b3b8-4bf1-87aa-6714df508a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Convert labels to one-hot encoded format\n",
        "train_arr = np.array(train['text'])\n",
        "train_lbl_arr = np.array(train['intent'])\n",
        "test_arr = np.array(test['text'])\n",
        "test_lbl_arr = np.array(test['intent'])\n",
        "\n",
        "train_lbl_encoded_arr = np.zeros((np.shape(train_lbl_arr)[0], len(all_classes)))\n",
        "print(np.shape(train_lbl_encoded_arr))\n",
        "\n",
        "test_lbl_encoded_arr = np.zeros((np.shape(test_lbl_arr)[0], len(all_classes)))\n",
        "print(np.shape(test_lbl_encoded_arr))\n",
        "\n",
        "for t in range(len(train_lbl_arr)):\n",
        "  intent_str = train_lbl_arr[t]\n",
        "  if '+' in intent_str:\n",
        "    intents = intent_str.split('+')\n",
        "    for i in intents:      \n",
        "      train_lbl_encoded_arr[t, np.where(all_classes==i)[0][0]] = 1\n",
        "  else:    \n",
        "    train_lbl_encoded_arr[t, np.where(all_classes==intent_str)[0][0]] = 1\n",
        "\n",
        "for t in range(len(test_lbl_arr)):\n",
        "  intent_str = test_lbl_arr[t]\n",
        "  if '+' in intent_str:\n",
        "    intents = intent_str.split('+')\n",
        "    for i in intents:      \n",
        "      test_lbl_encoded_arr[t, np.where(all_classes==i)[0][0]] = 1\n",
        "  else:    \n",
        "    test_lbl_encoded_arr[t, np.where(all_classes==intent_str)[0][0]] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4634, 18)\n",
            "(850, 18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdaQAG9_K4sj",
        "colab_type": "code",
        "outputId": "fd2a4ad5-7138-4771-954d-de1842d93e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def preprocess(s, return_as_tokenized = False):\n",
        "  # Make lowercase and tokenize\n",
        "  s = word_tokenize(s.lower())\n",
        "\n",
        "  # Replace numerical values with a #NUM# identifier\n",
        "  s = [('#NUM#' if t.isnumeric() else t) for t in s]\n",
        "\n",
        "  if (return_as_tokenized):\n",
        "    return s\n",
        "    \n",
        "  return ' '.join(s)\n",
        "\n",
        "train_arr_preprocessed = [preprocess(t) for t in train_arr]\n",
        "test_arr_preprocessed = [preprocess(t) for t in test_arr]\n",
        "print(np.shape(train_arr_preprocessed))\n",
        "print(np.shape(test_arr_preprocessed))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4634,)\n",
            "(850,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGfuxXfPAnP",
        "colab_type": "text"
      },
      "source": [
        "# Model-1: TF.IDF Feature with Only Dense Layered NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSaUc_K8PMPR",
        "colab_type": "text"
      },
      "source": [
        "## Create TF.IDF Feature Vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWfSpIqtPEdC",
        "colab_type": "code",
        "outputId": "9e1fc18d-8581-4327-ec5f-31184befd294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "x_train_tfidf = vectorizer.fit_transform(train_arr_preprocessed)\n",
        "x_test_tfidf = vectorizer.transform(test_arr_preprocessed)\n",
        "\n",
        "x_train_tfidf = x_train_tfidf.todense()\n",
        "x_test_tfidf = x_test_tfidf.todense()\n",
        "\n",
        "print(np.shape(test_lbl_encoded_arr))\n",
        "print(np.shape(x_test_tfidf))\n",
        "print(np.shape(train_lbl_encoded_arr))\n",
        "print(np.shape(x_train_tfidf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(850, 18)\n",
            "(850, 722)\n",
            "(4634, 18)\n",
            "(4634, 722)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzazqG8LO5x6",
        "colab_type": "text"
      },
      "source": [
        "## Create Dense Layered Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfnAL6QKfw98",
        "colab_type": "code",
        "outputId": "dec3b199-e7f0-4306-ae28-b5c851ed1f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, GRU, Embedding, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='tanh', input_dim=np.shape(x_train_tfidf)[1]))\n",
        "model.add(Dense(200, activation='tanh'))\n",
        "model.add(Dense(18, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_tfidf, train_lbl_encoded_arr, epochs=150, batch_size=100, verbose=2, validation_data=(x_test_tfidf, test_lbl_encoded_arr))\n",
        "\n",
        "# Calculate Label Prediction Accuracy\n",
        "pred = model.predict(x_test_tfidf)\n",
        "total_label_erros = np.sum(np.abs(np.round(pred)-test_lbl_encoded_arr))\n",
        "total_label_count = np.sum(test_lbl_encoded_arr)\n",
        "label_accuracy = total_label_erros/total_label_count\n",
        "1-label_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4634 samples, validate on 850 samples\n",
            "Epoch 1/150\n",
            " - 16s - loss: 0.2671 - acc: 0.9482 - val_loss: 0.0940 - val_acc: 0.9699\n",
            "Epoch 2/150\n",
            " - 0s - loss: 0.0748 - acc: 0.9756 - val_loss: 0.0734 - val_acc: 0.9785\n",
            "Epoch 3/150\n",
            " - 0s - loss: 0.0543 - acc: 0.9833 - val_loss: 0.0618 - val_acc: 0.9814\n",
            "Epoch 4/150\n",
            " - 1s - loss: 0.0410 - acc: 0.9881 - val_loss: 0.0538 - val_acc: 0.9835\n",
            "Epoch 5/150\n",
            " - 0s - loss: 0.0318 - acc: 0.9907 - val_loss: 0.0480 - val_acc: 0.9864\n",
            "Epoch 6/150\n",
            " - 0s - loss: 0.0250 - acc: 0.9928 - val_loss: 0.0444 - val_acc: 0.9872\n",
            "Epoch 7/150\n",
            " - 0s - loss: 0.0201 - acc: 0.9943 - val_loss: 0.0413 - val_acc: 0.9872\n",
            "Epoch 8/150\n",
            " - 0s - loss: 0.0168 - acc: 0.9953 - val_loss: 0.0389 - val_acc: 0.9875\n",
            "Epoch 9/150\n",
            " - 0s - loss: 0.0144 - acc: 0.9960 - val_loss: 0.0373 - val_acc: 0.9879\n",
            "Epoch 10/150\n",
            " - 0s - loss: 0.0122 - acc: 0.9968 - val_loss: 0.0353 - val_acc: 0.9884\n",
            "Epoch 11/150\n",
            " - 0s - loss: 0.0107 - acc: 0.9971 - val_loss: 0.0344 - val_acc: 0.9889\n",
            "Epoch 12/150\n",
            " - 0s - loss: 0.0095 - acc: 0.9974 - val_loss: 0.0322 - val_acc: 0.9897\n",
            "Epoch 13/150\n",
            " - 0s - loss: 0.0085 - acc: 0.9977 - val_loss: 0.0322 - val_acc: 0.9895\n",
            "Epoch 14/150\n",
            " - 0s - loss: 0.0076 - acc: 0.9980 - val_loss: 0.0304 - val_acc: 0.9907\n",
            "Epoch 15/150\n",
            " - 0s - loss: 0.0068 - acc: 0.9984 - val_loss: 0.0309 - val_acc: 0.9901\n",
            "Epoch 16/150\n",
            " - 0s - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0300 - val_acc: 0.9905\n",
            "Epoch 17/150\n",
            " - 0s - loss: 0.0056 - acc: 0.9986 - val_loss: 0.0297 - val_acc: 0.9902\n",
            "Epoch 18/150\n",
            " - 0s - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0284 - val_acc: 0.9912\n",
            "Epoch 19/150\n",
            " - 0s - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0287 - val_acc: 0.9908\n",
            "Epoch 20/150\n",
            " - 0s - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0289 - val_acc: 0.9908\n",
            "Epoch 21/150\n",
            " - 0s - loss: 0.0039 - acc: 0.9991 - val_loss: 0.0288 - val_acc: 0.9910\n",
            "Epoch 22/150\n",
            " - 0s - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0288 - val_acc: 0.9909\n",
            "Epoch 23/150\n",
            " - 0s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0294 - val_acc: 0.9908\n",
            "Epoch 24/150\n",
            " - 0s - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0289 - val_acc: 0.9910\n",
            "Epoch 25/150\n",
            " - 0s - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0280 - val_acc: 0.9914\n",
            "Epoch 26/150\n",
            " - 0s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0290 - val_acc: 0.9910\n",
            "Epoch 27/150\n",
            " - 0s - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0288 - val_acc: 0.9911\n",
            "Epoch 28/150\n",
            " - 0s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0291 - val_acc: 0.9912\n",
            "Epoch 29/150\n",
            " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0304 - val_acc: 0.9909\n",
            "Epoch 30/150\n",
            " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0295 - val_acc: 0.9911\n",
            "Epoch 31/150\n",
            " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0299 - val_acc: 0.9908\n",
            "Epoch 32/150\n",
            " - 0s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0304 - val_acc: 0.9909\n",
            "Epoch 33/150\n",
            " - 0s - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0307 - val_acc: 0.9910\n",
            "Epoch 34/150\n",
            " - 0s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0293 - val_acc: 0.9912\n",
            "Epoch 35/150\n",
            " - 0s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0307 - val_acc: 0.9910\n",
            "Epoch 36/150\n",
            " - 0s - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0306 - val_acc: 0.9911\n",
            "Epoch 37/150\n",
            " - 0s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0318 - val_acc: 0.9909\n",
            "Epoch 38/150\n",
            " - 0s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0307 - val_acc: 0.9912\n",
            "Epoch 39/150\n",
            " - 0s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0315 - val_acc: 0.9908\n",
            "Epoch 40/150\n",
            " - 0s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0318 - val_acc: 0.9910\n",
            "Epoch 41/150\n",
            " - 0s - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0314 - val_acc: 0.9913\n",
            "Epoch 42/150\n",
            " - 0s - loss: 9.5944e-04 - acc: 0.9998 - val_loss: 0.0319 - val_acc: 0.9911\n",
            "Epoch 43/150\n",
            " - 0s - loss: 9.2178e-04 - acc: 0.9999 - val_loss: 0.0326 - val_acc: 0.9912\n",
            "Epoch 44/150\n",
            " - 0s - loss: 8.3237e-04 - acc: 0.9999 - val_loss: 0.0322 - val_acc: 0.9912\n",
            "Epoch 45/150\n",
            " - 0s - loss: 7.7995e-04 - acc: 0.9999 - val_loss: 0.0318 - val_acc: 0.9911\n",
            "Epoch 46/150\n",
            " - 0s - loss: 7.8948e-04 - acc: 0.9999 - val_loss: 0.0326 - val_acc: 0.9913\n",
            "Epoch 47/150\n",
            " - 0s - loss: 7.1662e-04 - acc: 0.9999 - val_loss: 0.0337 - val_acc: 0.9912\n",
            "Epoch 48/150\n",
            " - 0s - loss: 6.6812e-04 - acc: 0.9999 - val_loss: 0.0330 - val_acc: 0.9914\n",
            "Epoch 49/150\n",
            " - 0s - loss: 6.2621e-04 - acc: 0.9999 - val_loss: 0.0329 - val_acc: 0.9911\n",
            "Epoch 50/150\n",
            " - 0s - loss: 6.0142e-04 - acc: 0.9999 - val_loss: 0.0334 - val_acc: 0.9912\n",
            "Epoch 51/150\n",
            " - 0s - loss: 5.4178e-04 - acc: 0.9999 - val_loss: 0.0353 - val_acc: 0.9913\n",
            "Epoch 52/150\n",
            " - 0s - loss: 5.2616e-04 - acc: 0.9999 - val_loss: 0.0339 - val_acc: 0.9914\n",
            "Epoch 53/150\n",
            " - 0s - loss: 4.9233e-04 - acc: 1.0000 - val_loss: 0.0339 - val_acc: 0.9914\n",
            "Epoch 54/150\n",
            " - 0s - loss: 4.5502e-04 - acc: 1.0000 - val_loss: 0.0338 - val_acc: 0.9915\n",
            "Epoch 55/150\n",
            " - 0s - loss: 4.5139e-04 - acc: 0.9999 - val_loss: 0.0346 - val_acc: 0.9913\n",
            "Epoch 56/150\n",
            " - 0s - loss: 4.2390e-04 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 0.9916\n",
            "Epoch 57/150\n",
            " - 0s - loss: 4.1172e-04 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 0.9916\n",
            "Epoch 58/150\n",
            " - 0s - loss: 3.9893e-04 - acc: 1.0000 - val_loss: 0.0360 - val_acc: 0.9916\n",
            "Epoch 59/150\n",
            " - 0s - loss: 3.6892e-04 - acc: 1.0000 - val_loss: 0.0353 - val_acc: 0.9914\n",
            "Epoch 60/150\n",
            " - 0s - loss: 3.5226e-04 - acc: 1.0000 - val_loss: 0.0359 - val_acc: 0.9916\n",
            "Epoch 61/150\n",
            " - 0s - loss: 3.4659e-04 - acc: 1.0000 - val_loss: 0.0363 - val_acc: 0.9919\n",
            "Epoch 62/150\n",
            " - 0s - loss: 3.2693e-04 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 0.9913\n",
            "Epoch 63/150\n",
            " - 0s - loss: 3.1492e-04 - acc: 1.0000 - val_loss: 0.0356 - val_acc: 0.9916\n",
            "Epoch 64/150\n",
            " - 0s - loss: 3.0327e-04 - acc: 1.0000 - val_loss: 0.0365 - val_acc: 0.9915\n",
            "Epoch 65/150\n",
            " - 0s - loss: 2.8225e-04 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9917\n",
            "Epoch 66/150\n",
            " - 0s - loss: 2.9374e-04 - acc: 1.0000 - val_loss: 0.0374 - val_acc: 0.9918\n",
            "Epoch 67/150\n",
            " - 0s - loss: 2.8214e-04 - acc: 1.0000 - val_loss: 0.0372 - val_acc: 0.9917\n",
            "Epoch 68/150\n",
            " - 0s - loss: 2.5197e-04 - acc: 1.0000 - val_loss: 0.0369 - val_acc: 0.9916\n",
            "Epoch 69/150\n",
            " - 0s - loss: 2.5271e-04 - acc: 1.0000 - val_loss: 0.0370 - val_acc: 0.9918\n",
            "Epoch 70/150\n",
            " - 0s - loss: 2.3415e-04 - acc: 1.0000 - val_loss: 0.0378 - val_acc: 0.9917\n",
            "Epoch 71/150\n",
            " - 0s - loss: 2.3562e-04 - acc: 1.0000 - val_loss: 0.0381 - val_acc: 0.9917\n",
            "Epoch 72/150\n",
            " - 0s - loss: 2.1372e-04 - acc: 1.0000 - val_loss: 0.0379 - val_acc: 0.9918\n",
            "Epoch 73/150\n",
            " - 0s - loss: 2.2792e-04 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9917\n",
            "Epoch 74/150\n",
            " - 0s - loss: 2.1262e-04 - acc: 1.0000 - val_loss: 0.0387 - val_acc: 0.9917\n",
            "Epoch 75/150\n",
            " - 0s - loss: 2.1360e-04 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9918\n",
            "Epoch 76/150\n",
            " - 0s - loss: 1.8624e-04 - acc: 1.0000 - val_loss: 0.0385 - val_acc: 0.9919\n",
            "Epoch 77/150\n",
            " - 0s - loss: 1.9708e-04 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9917\n",
            "Epoch 78/150\n",
            " - 0s - loss: 2.3946e-04 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 0.9918\n",
            "Epoch 79/150\n",
            " - 0s - loss: 2.1623e-04 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 0.9916\n",
            "Epoch 80/150\n",
            " - 0s - loss: 1.7845e-04 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 0.9919\n",
            "Epoch 81/150\n",
            " - 0s - loss: 1.8249e-04 - acc: 0.9999 - val_loss: 0.0391 - val_acc: 0.9917\n",
            "Epoch 82/150\n",
            " - 0s - loss: 1.6444e-04 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 0.9920\n",
            "Epoch 83/150\n",
            " - 0s - loss: 1.5270e-04 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 0.9918\n",
            "Epoch 84/150\n",
            " - 0s - loss: 1.4478e-04 - acc: 1.0000 - val_loss: 0.0398 - val_acc: 0.9917\n",
            "Epoch 85/150\n",
            " - 0s - loss: 1.4086e-04 - acc: 1.0000 - val_loss: 0.0397 - val_acc: 0.9918\n",
            "Epoch 86/150\n",
            " - 0s - loss: 1.3923e-04 - acc: 1.0000 - val_loss: 0.0392 - val_acc: 0.9918\n",
            "Epoch 87/150\n",
            " - 0s - loss: 1.4038e-04 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9918\n",
            "Epoch 88/150\n",
            " - 0s - loss: 1.3984e-04 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 0.9919\n",
            "Epoch 89/150\n",
            " - 0s - loss: 1.2658e-04 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9917\n",
            "Epoch 90/150\n",
            " - 0s - loss: 1.2729e-04 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9918\n",
            "Epoch 91/150\n",
            " - 0s - loss: 1.2995e-04 - acc: 1.0000 - val_loss: 0.0409 - val_acc: 0.9917\n",
            "Epoch 92/150\n",
            " - 0s - loss: 1.3217e-04 - acc: 1.0000 - val_loss: 0.0404 - val_acc: 0.9918\n",
            "Epoch 93/150\n",
            " - 0s - loss: 1.1491e-04 - acc: 1.0000 - val_loss: 0.0404 - val_acc: 0.9917\n",
            "Epoch 94/150\n",
            " - 0s - loss: 1.1553e-04 - acc: 1.0000 - val_loss: 0.0408 - val_acc: 0.9918\n",
            "Epoch 95/150\n",
            " - 0s - loss: 1.0143e-04 - acc: 1.0000 - val_loss: 0.0407 - val_acc: 0.9918\n",
            "Epoch 96/150\n",
            " - 0s - loss: 1.0416e-04 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9920\n",
            "Epoch 97/150\n",
            " - 0s - loss: 1.0552e-04 - acc: 1.0000 - val_loss: 0.0406 - val_acc: 0.9918\n",
            "Epoch 98/150\n",
            " - 0s - loss: 1.0179e-04 - acc: 1.0000 - val_loss: 0.0403 - val_acc: 0.9920\n",
            "Epoch 99/150\n",
            " - 0s - loss: 9.9265e-05 - acc: 1.0000 - val_loss: 0.0410 - val_acc: 0.9919\n",
            "Epoch 100/150\n",
            " - 0s - loss: 1.0409e-04 - acc: 1.0000 - val_loss: 0.0409 - val_acc: 0.9920\n",
            "Epoch 101/150\n",
            " - 0s - loss: 9.0702e-05 - acc: 1.0000 - val_loss: 0.0410 - val_acc: 0.9918\n",
            "Epoch 102/150\n",
            " - 0s - loss: 1.0352e-04 - acc: 1.0000 - val_loss: 0.0413 - val_acc: 0.9918\n",
            "Epoch 103/150\n",
            " - 0s - loss: 8.5558e-05 - acc: 1.0000 - val_loss: 0.0420 - val_acc: 0.9918\n",
            "Epoch 104/150\n",
            " - 0s - loss: 1.6144e-04 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 0.9918\n",
            "Epoch 105/150\n",
            " - 0s - loss: 8.6458e-05 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9918\n",
            "Epoch 106/150\n",
            " - 0s - loss: 8.1580e-05 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9920\n",
            "Epoch 107/150\n",
            " - 0s - loss: 8.3658e-05 - acc: 1.0000 - val_loss: 0.0421 - val_acc: 0.9920\n",
            "Epoch 108/150\n",
            " - 0s - loss: 7.7410e-05 - acc: 1.0000 - val_loss: 0.0421 - val_acc: 0.9918\n",
            "Epoch 109/150\n",
            " - 0s - loss: 7.9903e-05 - acc: 1.0000 - val_loss: 0.0419 - val_acc: 0.9920\n",
            "Epoch 110/150\n",
            " - 0s - loss: 7.9111e-05 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9920\n",
            "Epoch 111/150\n",
            " - 0s - loss: 7.9871e-05 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 0.9921\n",
            "Epoch 112/150\n",
            " - 0s - loss: 7.2580e-05 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9920\n",
            "Epoch 113/150\n",
            " - 0s - loss: 6.6750e-05 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 0.9920\n",
            "Epoch 114/150\n",
            " - 0s - loss: 6.2447e-05 - acc: 1.0000 - val_loss: 0.0428 - val_acc: 0.9920\n",
            "Epoch 115/150\n",
            " - 0s - loss: 6.6683e-05 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9921\n",
            "Epoch 116/150\n",
            " - 0s - loss: 6.3145e-05 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9920\n",
            "Epoch 117/150\n",
            " - 0s - loss: 5.9447e-05 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 0.9921\n",
            "Epoch 118/150\n",
            " - 0s - loss: 5.8053e-05 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 0.9921\n",
            "Epoch 119/150\n",
            " - 0s - loss: 5.2372e-05 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 0.9922\n",
            "Epoch 120/150\n",
            " - 0s - loss: 5.7823e-05 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 0.9922\n",
            "Epoch 121/150\n",
            " - 0s - loss: 5.2855e-05 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9922\n",
            "Epoch 122/150\n",
            " - 0s - loss: 5.9645e-05 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9921\n",
            "Epoch 123/150\n",
            " - 0s - loss: 5.5850e-05 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 0.9921\n",
            "Epoch 124/150\n",
            " - 0s - loss: 4.9335e-05 - acc: 1.0000 - val_loss: 0.0436 - val_acc: 0.9922\n",
            "Epoch 125/150\n",
            " - 0s - loss: 4.8786e-05 - acc: 1.0000 - val_loss: 0.0439 - val_acc: 0.9920\n",
            "Epoch 126/150\n",
            " - 0s - loss: 4.7475e-05 - acc: 1.0000 - val_loss: 0.0439 - val_acc: 0.9922\n",
            "Epoch 127/150\n",
            " - 0s - loss: 4.1253e-05 - acc: 1.0000 - val_loss: 0.0438 - val_acc: 0.9922\n",
            "Epoch 128/150\n",
            " - 0s - loss: 4.2369e-05 - acc: 1.0000 - val_loss: 0.0441 - val_acc: 0.9921\n",
            "Epoch 129/150\n",
            " - 0s - loss: 4.9612e-05 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 0.9922\n",
            "Epoch 130/150\n",
            " - 0s - loss: 4.0416e-05 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 0.9921\n",
            "Epoch 131/150\n",
            " - 0s - loss: 3.9008e-05 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 0.9922\n",
            "Epoch 132/150\n",
            " - 0s - loss: 4.3163e-05 - acc: 1.0000 - val_loss: 0.0443 - val_acc: 0.9921\n",
            "Epoch 133/150\n",
            " - 0s - loss: 4.0920e-05 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 0.9922\n",
            "Epoch 134/150\n",
            " - 0s - loss: 3.6808e-05 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 0.9921\n",
            "Epoch 135/150\n",
            " - 0s - loss: 3.0641e-05 - acc: 1.0000 - val_loss: 0.0442 - val_acc: 0.9922\n",
            "Epoch 136/150\n",
            " - 0s - loss: 2.9225e-05 - acc: 1.0000 - val_loss: 0.0441 - val_acc: 0.9922\n",
            "Epoch 137/150\n",
            " - 0s - loss: 3.1541e-05 - acc: 1.0000 - val_loss: 0.0446 - val_acc: 0.9922\n",
            "Epoch 138/150\n",
            " - 0s - loss: 6.3726e-05 - acc: 1.0000 - val_loss: 0.0443 - val_acc: 0.9922\n",
            "Epoch 139/150\n",
            " - 0s - loss: 5.4033e-05 - acc: 1.0000 - val_loss: 0.0435 - val_acc: 0.9923\n",
            "Epoch 140/150\n",
            " - 0s - loss: 6.3313e-05 - acc: 1.0000 - val_loss: 0.0451 - val_acc: 0.9920\n",
            "Epoch 141/150\n",
            " - 0s - loss: 5.9440e-05 - acc: 1.0000 - val_loss: 0.0441 - val_acc: 0.9923\n",
            "Epoch 142/150\n",
            " - 0s - loss: 4.4605e-05 - acc: 1.0000 - val_loss: 0.0452 - val_acc: 0.9920\n",
            "Epoch 143/150\n",
            " - 0s - loss: 9.4774e-05 - acc: 1.0000 - val_loss: 0.0442 - val_acc: 0.9920\n",
            "Epoch 144/150\n",
            " - 0s - loss: 8.8692e-05 - acc: 1.0000 - val_loss: 0.0460 - val_acc: 0.9921\n",
            "Epoch 145/150\n",
            " - 0s - loss: 9.5364e-05 - acc: 1.0000 - val_loss: 0.0442 - val_acc: 0.9921\n",
            "Epoch 146/150\n",
            " - 0s - loss: 5.0895e-05 - acc: 1.0000 - val_loss: 0.0441 - val_acc: 0.9920\n",
            "Epoch 147/150\n",
            " - 0s - loss: 9.4298e-05 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9914\n",
            "Epoch 148/150\n",
            " - 0s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0439 - val_acc: 0.9925\n",
            "Epoch 149/150\n",
            " - 0s - loss: 5.6167e-04 - acc: 0.9998 - val_loss: 0.0433 - val_acc: 0.9923\n",
            "Epoch 150/150\n",
            " - 0s - loss: 9.3549e-04 - acc: 0.9997 - val_loss: 0.0457 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8647398843930636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVqoWVzWsHF3",
        "colab_type": "text"
      },
      "source": [
        "## Calculate Overall Label Prediction Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17JmkHfbluL_",
        "colab_type": "code",
        "outputId": "9abb2b07-7fdf-49bb-f0ee-0772cd133ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate Label Prediction Accuracy\n",
        "pred = model.predict(x_test_tfidf)\n",
        "total_label_erros = np.sum(np.abs(np.round(pred)-test_lbl_encoded_arr))\n",
        "total_label_count = np.sum(test_lbl_encoded_arr)\n",
        "label_accuracy = total_label_erros/total_label_count\n",
        "1-label_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8647398843930636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVf2h3BKsM3w",
        "colab_type": "text"
      },
      "source": [
        "## Save Model Files And TF.IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXrwOCb8vavS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model and tokenizer\n",
        "import pickle\n",
        "\n",
        "model.save(\"tfidf_model.h5\")\n",
        "\n",
        "with open('vectorizer.npy', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNdCsQnotay3",
        "colab_type": "text"
      },
      "source": [
        "# Model-2 Neural Embedding Layer with RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi9vHy_Osfne",
        "colab_type": "text"
      },
      "source": [
        "## Word Tokenization & Padding For Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COHQY1uks_pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tüm kelimeleri derle ve numeric formatta ayrıştır\n",
        "vocabulary_size = 1000\n",
        "max_sentence_len = 50\n",
        "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
        "tokenizer.fit_on_texts(train_arr_preprocessed)\n",
        "\n",
        "train_embedding_inputs = tokenizer.texts_to_sequences(train_arr_preprocessed)\n",
        "train_embedding_inputs = pad_sequences(train_embedding_inputs, maxlen = max_sentence_len)\n",
        "\n",
        "test_embedding_inputs = tokenizer.texts_to_sequences(test_arr_preprocessed)\n",
        "test_embedding_inputs = pad_sequences(test_embedding_inputs, maxlen = max_sentence_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGickEBBstfG",
        "colab_type": "text"
      },
      "source": [
        "## Create RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhmEuwEJuLzC",
        "colab_type": "code",
        "outputId": "efcbd8fa-c951-4331-d1ed-ef3312278be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, GRU, Embedding     \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, 100, input_length=max_sentence_len))\n",
        "model.add(LSTM(10, return_sequences=False))\n",
        "model.add(Dense(200, activation='tanh'))\n",
        "model.add(Dense(200, activation='tanh'))\n",
        "model.add(Dense(18, activation='sigmoid')) #tanh\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " \n",
        "model.fit(train_embedding_inputs, train_lbl_encoded_arr, epochs=150, batch_size=100, verbose=2, validation_data=(test_embedding_inputs, test_lbl_encoded_arr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4634 samples, validate on 850 samples\n",
            "Epoch 1/150\n",
            " - 19s - loss: 0.2741 - acc: 0.9616 - val_loss: 0.1137 - val_acc: 0.9699\n",
            "Epoch 2/150\n",
            " - 2s - loss: 0.0932 - acc: 0.9712 - val_loss: 0.1008 - val_acc: 0.9699\n",
            "Epoch 3/150\n",
            " - 2s - loss: 0.0851 - acc: 0.9719 - val_loss: 0.0891 - val_acc: 0.9741\n",
            "Epoch 4/150\n",
            " - 2s - loss: 0.0728 - acc: 0.9776 - val_loss: 0.0766 - val_acc: 0.9795\n",
            "Epoch 5/150\n",
            " - 2s - loss: 0.0570 - acc: 0.9821 - val_loss: 0.0703 - val_acc: 0.9808\n",
            "Epoch 6/150\n",
            " - 2s - loss: 0.0471 - acc: 0.9850 - val_loss: 0.0653 - val_acc: 0.9822\n",
            "Epoch 7/150\n",
            " - 2s - loss: 0.0411 - acc: 0.9880 - val_loss: 0.0601 - val_acc: 0.9849\n",
            "Epoch 8/150\n",
            " - 2s - loss: 0.0348 - acc: 0.9904 - val_loss: 0.0575 - val_acc: 0.9867\n",
            "Epoch 9/150\n",
            " - 2s - loss: 0.0302 - acc: 0.9921 - val_loss: 0.0543 - val_acc: 0.9878\n",
            "Epoch 10/150\n",
            " - 2s - loss: 0.0260 - acc: 0.9929 - val_loss: 0.0485 - val_acc: 0.9893\n",
            "Epoch 11/150\n",
            " - 2s - loss: 0.0226 - acc: 0.9934 - val_loss: 0.0497 - val_acc: 0.9889\n",
            "Epoch 12/150\n",
            " - 2s - loss: 0.0200 - acc: 0.9942 - val_loss: 0.0469 - val_acc: 0.9898\n",
            "Epoch 13/150\n",
            " - 2s - loss: 0.0182 - acc: 0.9948 - val_loss: 0.0435 - val_acc: 0.9901\n",
            "Epoch 14/150\n",
            " - 2s - loss: 0.0162 - acc: 0.9952 - val_loss: 0.0425 - val_acc: 0.9904\n",
            "Epoch 15/150\n",
            " - 2s - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0415 - val_acc: 0.9904\n",
            "Epoch 16/150\n",
            " - 2s - loss: 0.0136 - acc: 0.9960 - val_loss: 0.0403 - val_acc: 0.9905\n",
            "Epoch 17/150\n",
            " - 2s - loss: 0.0125 - acc: 0.9962 - val_loss: 0.0390 - val_acc: 0.9907\n",
            "Epoch 18/150\n",
            " - 2s - loss: 0.0120 - acc: 0.9963 - val_loss: 0.0397 - val_acc: 0.9896\n",
            "Epoch 19/150\n",
            " - 3s - loss: 0.0115 - acc: 0.9966 - val_loss: 0.0378 - val_acc: 0.9911\n",
            "Epoch 20/150\n",
            " - 2s - loss: 0.0099 - acc: 0.9971 - val_loss: 0.0376 - val_acc: 0.9910\n",
            "Epoch 21/150\n",
            " - 3s - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0356 - val_acc: 0.9909\n",
            "Epoch 22/150\n",
            " - 2s - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0329 - val_acc: 0.9911\n",
            "Epoch 23/150\n",
            " - 2s - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0342 - val_acc: 0.9912\n",
            "Epoch 24/150\n",
            " - 2s - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0343 - val_acc: 0.9912\n",
            "Epoch 25/150\n",
            " - 2s - loss: 0.0065 - acc: 0.9983 - val_loss: 0.0326 - val_acc: 0.9918\n",
            "Epoch 26/150\n",
            " - 2s - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0337 - val_acc: 0.9913\n",
            "Epoch 27/150\n",
            " - 2s - loss: 0.0057 - acc: 0.9985 - val_loss: 0.0346 - val_acc: 0.9910\n",
            "Epoch 28/150\n",
            " - 2s - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0345 - val_acc: 0.9912\n",
            "Epoch 29/150\n",
            " - 2s - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0331 - val_acc: 0.9921\n",
            "Epoch 30/150\n",
            " - 2s - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0335 - val_acc: 0.9918\n",
            "Epoch 31/150\n",
            " - 2s - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0334 - val_acc: 0.9922\n",
            "Epoch 32/150\n",
            " - 2s - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0332 - val_acc: 0.9924\n",
            "Epoch 33/150\n",
            " - 2s - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0334 - val_acc: 0.9927\n",
            "Epoch 34/150\n",
            " - 2s - loss: 0.0032 - acc: 0.9994 - val_loss: 0.0334 - val_acc: 0.9927\n",
            "Epoch 35/150\n",
            " - 2s - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0326 - val_acc: 0.9929\n",
            "Epoch 36/150\n",
            " - 2s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0319 - val_acc: 0.9931\n",
            "Epoch 37/150\n",
            " - 2s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0316 - val_acc: 0.9930\n",
            "Epoch 38/150\n",
            " - 2s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0328 - val_acc: 0.9930\n",
            "Epoch 39/150\n",
            " - 2s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0330 - val_acc: 0.9931\n",
            "Epoch 40/150\n",
            " - 3s - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0318 - val_acc: 0.9933\n",
            "Epoch 41/150\n",
            " - 3s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0329 - val_acc: 0.9931\n",
            "Epoch 42/150\n",
            " - 2s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0342 - val_acc: 0.9927\n",
            "Epoch 43/150\n",
            " - 3s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0332 - val_acc: 0.9933\n",
            "Epoch 44/150\n",
            " - 2s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0354 - val_acc: 0.9927\n",
            "Epoch 45/150\n",
            " - 2s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0358 - val_acc: 0.9929\n",
            "Epoch 46/150\n",
            " - 2s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0329 - val_acc: 0.9935\n",
            "Epoch 47/150\n",
            " - 2s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0334 - val_acc: 0.9933\n",
            "Epoch 48/150\n",
            " - 2s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0344 - val_acc: 0.9928\n",
            "Epoch 49/150\n",
            " - 2s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0354 - val_acc: 0.9933\n",
            "Epoch 50/150\n",
            " - 2s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0342 - val_acc: 0.9934\n",
            "Epoch 51/150\n",
            " - 2s - loss: 8.6060e-04 - acc: 0.9999 - val_loss: 0.0337 - val_acc: 0.9936\n",
            "Epoch 52/150\n",
            " - 2s - loss: 7.7417e-04 - acc: 0.9999 - val_loss: 0.0348 - val_acc: 0.9933\n",
            "Epoch 53/150\n",
            " - 2s - loss: 7.0503e-04 - acc: 0.9999 - val_loss: 0.0343 - val_acc: 0.9935\n",
            "Epoch 54/150\n",
            " - 2s - loss: 6.5643e-04 - acc: 0.9999 - val_loss: 0.0346 - val_acc: 0.9937\n",
            "Epoch 55/150\n",
            " - 2s - loss: 5.8568e-04 - acc: 0.9999 - val_loss: 0.0353 - val_acc: 0.9934\n",
            "Epoch 56/150\n",
            " - 2s - loss: 5.6071e-04 - acc: 0.9999 - val_loss: 0.0351 - val_acc: 0.9935\n",
            "Epoch 57/150\n",
            " - 2s - loss: 5.2292e-04 - acc: 0.9999 - val_loss: 0.0348 - val_acc: 0.9937\n",
            "Epoch 58/150\n",
            " - 2s - loss: 5.1649e-04 - acc: 1.0000 - val_loss: 0.0350 - val_acc: 0.9935\n",
            "Epoch 59/150\n",
            " - 2s - loss: 4.4996e-04 - acc: 1.0000 - val_loss: 0.0355 - val_acc: 0.9937\n",
            "Epoch 60/150\n",
            " - 2s - loss: 7.3180e-04 - acc: 0.9999 - val_loss: 0.0344 - val_acc: 0.9936\n",
            "Epoch 61/150\n",
            " - 2s - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0357 - val_acc: 0.9929\n",
            "Epoch 62/150\n",
            " - 3s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0359 - val_acc: 0.9935\n",
            "Epoch 63/150\n",
            " - 2s - loss: 8.5095e-04 - acc: 0.9998 - val_loss: 0.0379 - val_acc: 0.9934\n",
            "Epoch 64/150\n",
            " - 2s - loss: 8.0364e-04 - acc: 0.9999 - val_loss: 0.0360 - val_acc: 0.9933\n",
            "Epoch 65/150\n",
            " - 2s - loss: 6.2147e-04 - acc: 0.9999 - val_loss: 0.0375 - val_acc: 0.9932\n",
            "Epoch 66/150\n",
            " - 2s - loss: 4.4891e-04 - acc: 1.0000 - val_loss: 0.0370 - val_acc: 0.9936\n",
            "Epoch 67/150\n",
            " - 2s - loss: 3.3493e-04 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 0.9937\n",
            "Epoch 68/150\n",
            " - 2s - loss: 2.9865e-04 - acc: 1.0000 - val_loss: 0.0371 - val_acc: 0.9937\n",
            "Epoch 69/150\n",
            " - 2s - loss: 2.7522e-04 - acc: 1.0000 - val_loss: 0.0374 - val_acc: 0.9937\n",
            "Epoch 70/150\n",
            " - 2s - loss: 2.5753e-04 - acc: 1.0000 - val_loss: 0.0374 - val_acc: 0.9937\n",
            "Epoch 71/150\n",
            " - 2s - loss: 2.4319e-04 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9937\n",
            "Epoch 72/150\n",
            " - 2s - loss: 2.2866e-04 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9938\n",
            "Epoch 73/150\n",
            " - 2s - loss: 2.1760e-04 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9938\n",
            "Epoch 74/150\n",
            " - 2s - loss: 2.0731e-04 - acc: 1.0000 - val_loss: 0.0379 - val_acc: 0.9938\n",
            "Epoch 75/150\n",
            " - 2s - loss: 1.9699e-04 - acc: 1.0000 - val_loss: 0.0381 - val_acc: 0.9938\n",
            "Epoch 76/150\n",
            " - 2s - loss: 1.8710e-04 - acc: 1.0000 - val_loss: 0.0384 - val_acc: 0.9939\n",
            "Epoch 77/150\n",
            " - 2s - loss: 1.8012e-04 - acc: 1.0000 - val_loss: 0.0382 - val_acc: 0.9937\n",
            "Epoch 78/150\n",
            " - 2s - loss: 1.7469e-04 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 0.9937\n",
            "Epoch 79/150\n",
            " - 2s - loss: 1.7341e-04 - acc: 1.0000 - val_loss: 0.0387 - val_acc: 0.9936\n",
            "Epoch 80/150\n",
            " - 2s - loss: 1.5666e-04 - acc: 1.0000 - val_loss: 0.0386 - val_acc: 0.9937\n",
            "Epoch 81/150\n",
            " - 2s - loss: 1.5180e-04 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 0.9936\n",
            "Epoch 82/150\n",
            " - 2s - loss: 1.4467e-04 - acc: 1.0000 - val_loss: 0.0386 - val_acc: 0.9937\n",
            "Epoch 83/150\n",
            " - 2s - loss: 1.3920e-04 - acc: 1.0000 - val_loss: 0.0393 - val_acc: 0.9936\n",
            "Epoch 84/150\n",
            " - 2s - loss: 1.4358e-04 - acc: 1.0000 - val_loss: 0.0383 - val_acc: 0.9938\n",
            "Epoch 85/150\n",
            " - 2s - loss: 1.6733e-04 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9938\n",
            "Epoch 86/150\n",
            " - 2s - loss: 1.4645e-04 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 0.9937\n",
            "Epoch 87/150\n",
            " - 2s - loss: 1.1808e-04 - acc: 1.0000 - val_loss: 0.0389 - val_acc: 0.9939\n",
            "Epoch 88/150\n",
            " - 2s - loss: 1.1123e-04 - acc: 1.0000 - val_loss: 0.0392 - val_acc: 0.9938\n",
            "Epoch 89/150\n",
            " - 2s - loss: 1.0642e-04 - acc: 1.0000 - val_loss: 0.0392 - val_acc: 0.9938\n",
            "Epoch 90/150\n",
            " - 2s - loss: 1.0158e-04 - acc: 1.0000 - val_loss: 0.0393 - val_acc: 0.9939\n",
            "Epoch 91/150\n",
            " - 2s - loss: 9.7594e-05 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 0.9939\n",
            "Epoch 92/150\n",
            " - 2s - loss: 9.3782e-05 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 0.9939\n",
            "Epoch 93/150\n",
            " - 2s - loss: 9.0039e-05 - acc: 1.0000 - val_loss: 0.0396 - val_acc: 0.9939\n",
            "Epoch 94/150\n",
            " - 2s - loss: 8.6860e-05 - acc: 1.0000 - val_loss: 0.0397 - val_acc: 0.9939\n",
            "Epoch 95/150\n",
            " - 2s - loss: 8.3523e-05 - acc: 1.0000 - val_loss: 0.0398 - val_acc: 0.9939\n",
            "Epoch 96/150\n",
            " - 2s - loss: 8.0684e-05 - acc: 1.0000 - val_loss: 0.0398 - val_acc: 0.9939\n",
            "Epoch 97/150\n",
            " - 2s - loss: 7.8951e-05 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 0.9939\n",
            "Epoch 98/150\n",
            " - 2s - loss: 7.4546e-05 - acc: 1.0000 - val_loss: 0.0400 - val_acc: 0.9939\n",
            "Epoch 99/150\n",
            " - 2s - loss: 7.1523e-05 - acc: 1.0000 - val_loss: 0.0401 - val_acc: 0.9939\n",
            "Epoch 100/150\n",
            " - 2s - loss: 6.8885e-05 - acc: 1.0000 - val_loss: 0.0403 - val_acc: 0.9939\n",
            "Epoch 101/150\n",
            " - 2s - loss: 6.6507e-05 - acc: 1.0000 - val_loss: 0.0403 - val_acc: 0.9939\n",
            "Epoch 102/150\n",
            " - 2s - loss: 6.4216e-05 - acc: 1.0000 - val_loss: 0.0405 - val_acc: 0.9939\n",
            "Epoch 103/150\n",
            " - 2s - loss: 6.2190e-05 - acc: 1.0000 - val_loss: 0.0405 - val_acc: 0.9938\n",
            "Epoch 104/150\n",
            " - 2s - loss: 6.0002e-05 - acc: 1.0000 - val_loss: 0.0406 - val_acc: 0.9939\n",
            "Epoch 105/150\n",
            " - 2s - loss: 5.8315e-05 - acc: 1.0000 - val_loss: 0.0406 - val_acc: 0.9938\n",
            "Epoch 106/150\n",
            " - 2s - loss: 5.6607e-05 - acc: 1.0000 - val_loss: 0.0409 - val_acc: 0.9939\n",
            "Epoch 107/150\n",
            " - 2s - loss: 5.4779e-05 - acc: 1.0000 - val_loss: 0.0407 - val_acc: 0.9938\n",
            "Epoch 108/150\n",
            " - 2s - loss: 5.3278e-05 - acc: 1.0000 - val_loss: 0.0411 - val_acc: 0.9939\n",
            "Epoch 109/150\n",
            " - 2s - loss: 5.1928e-05 - acc: 1.0000 - val_loss: 0.0411 - val_acc: 0.9939\n",
            "Epoch 110/150\n",
            " - 2s - loss: 4.9737e-05 - acc: 1.0000 - val_loss: 0.0413 - val_acc: 0.9939\n",
            "Epoch 111/150\n",
            " - 2s - loss: 4.8098e-05 - acc: 1.0000 - val_loss: 0.0412 - val_acc: 0.9939\n",
            "Epoch 112/150\n",
            " - 3s - loss: 5.7375e-05 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9939\n",
            "Epoch 113/150\n",
            " - 2s - loss: 8.2922e-05 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9935\n",
            "Epoch 114/150\n",
            " - 2s - loss: 5.7571e-05 - acc: 1.0000 - val_loss: 0.0416 - val_acc: 0.9938\n",
            "Epoch 115/150\n",
            " - 2s - loss: 4.4487e-05 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 0.9940\n",
            "Epoch 116/150\n",
            " - 2s - loss: 4.1658e-05 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 0.9939\n",
            "Epoch 117/150\n",
            " - 2s - loss: 3.9673e-05 - acc: 1.0000 - val_loss: 0.0417 - val_acc: 0.9939\n",
            "Epoch 118/150\n",
            " - 2s - loss: 3.8331e-05 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9939\n",
            "Epoch 119/150\n",
            " - 2s - loss: 3.7081e-05 - acc: 1.0000 - val_loss: 0.0419 - val_acc: 0.9939\n",
            "Epoch 120/150\n",
            " - 2s - loss: 3.5850e-05 - acc: 1.0000 - val_loss: 0.0420 - val_acc: 0.9939\n",
            "Epoch 121/150\n",
            " - 2s - loss: 3.4850e-05 - acc: 1.0000 - val_loss: 0.0421 - val_acc: 0.9939\n",
            "Epoch 122/150\n",
            " - 2s - loss: 3.3757e-05 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9939\n",
            "Epoch 123/150\n",
            " - 2s - loss: 3.2720e-05 - acc: 1.0000 - val_loss: 0.0424 - val_acc: 0.9939\n",
            "Epoch 124/150\n",
            " - 2s - loss: 3.1828e-05 - acc: 1.0000 - val_loss: 0.0424 - val_acc: 0.9939\n",
            "Epoch 125/150\n",
            " - 2s - loss: 3.0888e-05 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9940\n",
            "Epoch 126/150\n",
            " - 2s - loss: 3.0059e-05 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9939\n",
            "Epoch 127/150\n",
            " - 2s - loss: 2.9091e-05 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9941\n",
            "Epoch 128/150\n",
            " - 2s - loss: 2.8340e-05 - acc: 1.0000 - val_loss: 0.0428 - val_acc: 0.9940\n",
            "Epoch 129/150\n",
            " - 2s - loss: 2.7588e-05 - acc: 1.0000 - val_loss: 0.0429 - val_acc: 0.9940\n",
            "Epoch 130/150\n",
            " - 2s - loss: 2.6742e-05 - acc: 1.0000 - val_loss: 0.0430 - val_acc: 0.9940\n",
            "Epoch 131/150\n",
            " - 2s - loss: 2.5987e-05 - acc: 1.0000 - val_loss: 0.0430 - val_acc: 0.9940\n",
            "Epoch 132/150\n",
            " - 2s - loss: 2.5272e-05 - acc: 1.0000 - val_loss: 0.0432 - val_acc: 0.9940\n",
            "Epoch 133/150\n",
            " - 2s - loss: 2.4653e-05 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 0.9941\n",
            "Epoch 134/150\n",
            " - 2s - loss: 2.4053e-05 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 0.9939\n",
            "Epoch 135/150\n",
            " - 2s - loss: 2.3335e-05 - acc: 1.0000 - val_loss: 0.0433 - val_acc: 0.9940\n",
            "Epoch 136/150\n",
            " - 2s - loss: 2.2769e-05 - acc: 1.0000 - val_loss: 0.0435 - val_acc: 0.9939\n",
            "Epoch 137/150\n",
            " - 2s - loss: 2.2376e-05 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 0.9941\n",
            "Epoch 138/150\n",
            " - 2s - loss: 2.2341e-05 - acc: 1.0000 - val_loss: 0.0440 - val_acc: 0.9938\n",
            "Epoch 139/150\n",
            " - 2s - loss: 2.3597e-05 - acc: 1.0000 - val_loss: 0.0430 - val_acc: 0.9940\n",
            "Epoch 140/150\n",
            " - 2s - loss: 6.2230e-04 - acc: 0.9998 - val_loss: 0.0539 - val_acc: 0.9914\n",
            "Epoch 141/150\n",
            " - 2s - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0458 - val_acc: 0.9922\n",
            "Epoch 142/150\n",
            " - 2s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0358 - val_acc: 0.9937\n",
            "Epoch 143/150\n",
            " - 2s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0428 - val_acc: 0.9936\n",
            "Epoch 144/150\n",
            " - 2s - loss: 6.8759e-04 - acc: 0.9998 - val_loss: 0.0396 - val_acc: 0.9935\n",
            "Epoch 145/150\n",
            " - 2s - loss: 2.3801e-04 - acc: 1.0000 - val_loss: 0.0385 - val_acc: 0.9939\n",
            "Epoch 146/150\n",
            " - 2s - loss: 1.3693e-04 - acc: 1.0000 - val_loss: 0.0404 - val_acc: 0.9937\n",
            "Epoch 147/150\n",
            " - 2s - loss: 8.4022e-05 - acc: 1.0000 - val_loss: 0.0395 - val_acc: 0.9940\n",
            "Epoch 148/150\n",
            " - 2s - loss: 7.1904e-05 - acc: 1.0000 - val_loss: 0.0404 - val_acc: 0.9938\n",
            "Epoch 149/150\n",
            " - 2s - loss: 6.3376e-05 - acc: 1.0000 - val_loss: 0.0403 - val_acc: 0.9939\n",
            "Epoch 150/150\n",
            " - 2s - loss: 5.7275e-05 - acc: 1.0000 - val_loss: 0.0405 - val_acc: 0.9939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc44ceca7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5lUWs2sw9_",
        "colab_type": "text"
      },
      "source": [
        "## Calculate Overall Label Prediction Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlL1Dc4Ftzaz",
        "colab_type": "code",
        "outputId": "a7e2b41b-58f1-4448-bfa8-ba374f686755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate Label Prediction Accuracy\n",
        "pred = model.predict(test_embedding_inputs)\n",
        "total_label_erros = np.sum(np.abs(np.round(pred)-test_lbl_encoded_arr))\n",
        "total_label_count = np.sum(test_lbl_encoded_arr)\n",
        "label_accuracy = total_label_erros/total_label_count\n",
        "1-label_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8705202312138729"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkk1sDTc542h",
        "colab_type": "code",
        "outputId": "f51e41c3-6dda-407b-bc7b-8f7e1eccbbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(test_embedding_inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(850, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CmMDRqSs67I",
        "colab_type": "text"
      },
      "source": [
        "## Savel Model Files And Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij7fm1ywuy-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Save model and tokenizer\n",
        "import pickle\n",
        "\n",
        "model.save(\"rnn_model.h5\")\n",
        "\n",
        "with open('rnn_tokenizer.npy', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}